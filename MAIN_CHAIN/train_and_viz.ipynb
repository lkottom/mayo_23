{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_dataset import CustomDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "import time \n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from model_vae_8_dim import Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_images(images):\n",
    "#     \"\"\"Normalizes images to the range [-1, 1].\"\"\"\n",
    "#     images = images.clone()\n",
    "#     images -= images.min()\n",
    "#     images /= images.max()\n",
    "#     images *= 2.0\n",
    "#     images -= 1.0\n",
    "#     return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def example_images(original_images, reconstructed_images, epoch, batch_idx, log_interval):\n",
    "    n_samples = 5  # Number of samples to visualize\n",
    "\n",
    "    original_images = original_images[:n_samples].cpu()\n",
    "    reconstructed_images = reconstructed_images[:n_samples].cpu().detach() \n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(n_samples, 2, figsize=(10, 10))\n",
    "    for i in range(n_samples):\n",
    "        axes[i, 0].imshow(original_images[i].permute(1, 2, 0))  # Rearrange channels\n",
    "        axes[i, 0].set_title('Original')\n",
    "        axes[i, 0].axis('off')\n",
    "    \n",
    "        axes[i, 1].imshow(reconstructed_images[i].permute(1, 2, 0))  # Rearrange channels\n",
    "        axes[i, 1].set_title('Reconstructed')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    os.makedirs('TRAINING_IMAGES_64_TEST')\n",
    "    plt.savefig(f'TRAINING_IMAGES_64_TEST/reconstruction_examples_epoch_{epoch}_batch_{batch_idx * log_interval}.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This script is an example of VAE training in PyTorch. The code was adapted from:\n",
    "https://github.com/pytorch/examples/blob/master/vae/main.py \"\"\"\n",
    "\n",
    "## Arguments\n",
    "args = argparse.Namespace(\n",
    "    batch_size=64,\n",
    "    input_size=64,\n",
    "    epochs=15,\n",
    "    no_cuda=False,\n",
    "    log_interval=500,\n",
    "    model='mse_vae',\n",
    "    log_dir='mse_vae'\n",
    ")\n",
    "\n",
    "\n",
    "## Cuda\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:3\" if args.cuda else \"cpu\")\n",
    "\n",
    "# ## Data Parallelism\n",
    "# if args.cuda and torch.cuda.device_count() > 1:\n",
    "#     print(\"Using\", torch.cuda.device_count(), \"GPUs for Data Parallelism!\")\n",
    "#     model = nn.DataParallel(Autoencoder(args.input_size)).to(device)\n",
    "# # else:\n",
    "model = Autoencoder(args.input_size).to(device)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 48, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "mean = (0.5, 0.5, 0.5)\n",
    "std = (0.5, 0.5, 0.5)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((args.input_size, args.input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "def random_mask(img_data, device):\n",
    "    # Generate a binary mask with a random percentage of values set to 1\n",
    "    mask_percentage = np.random.uniform(0.05, 0.2)\n",
    "    mask = torch.rand(img_data.shape, device=device) < mask_percentage\n",
    "    # Apply the mask to the random_image to replace values with -1000\n",
    "    dummy_value = -1000\n",
    "    masked_image = torch.where(mask, dummy_value, img_data)\n",
    "    return masked_image\n",
    "        \n",
    "# ## ---------------------------------------------------\n",
    "train_dataset = CustomDataset(data_path='/mayo_atlas/home/m296984/RESULTS_40x/Liver/test_64x64_patches', transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "## Logging\n",
    "os.makedirs('vae_logs/{}'.format(args.log_dir), exist_ok=True)\n",
    "summary_writer = SummaryWriter(log_dir='vae_logs/' + args.log_dir, purge_step=0)\n",
    "\n",
    "## Build Model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/190782 (0%)]\tMSE: 0.004061\tKL: 0.319830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [32000/190782 (17%)]\tMSE: 0.002721\tKL: 0.184336\n",
      "Train Epoch: 1 [64000/190782 (34%)]\tMSE: 0.002599\tKL: 0.067686\n",
      "Train Epoch: 1 [96000/190782 (50%)]\tMSE: 0.002644\tKL: 0.000331\n",
      "Train Epoch: 1 [128000/190782 (67%)]\tMSE: 0.002789\tKL: 0.000205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [160000/190782 (84%)]\tMSE: 0.002799\tKL: 0.000345\n",
      "====> Epoch: 1 Average loss: 0.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [0/190782 (0%)]\tMSE: 0.002497\tKL: 0.000237\n",
      "Train Epoch: 2 [32000/190782 (17%)]\tMSE: 0.002686\tKL: 0.000214\n",
      "Train Epoch: 2 [64000/190782 (34%)]\tMSE: 0.002704\tKL: 0.000228\n",
      "Train Epoch: 2 [96000/190782 (50%)]\tMSE: 0.002713\tKL: 0.000105\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        original_shape = data.size()\n",
    "        data = data.view(data.size(0), -1)\n",
    "        data = data.to(device)\n",
    "        masked_image = random_mask(data, device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run VAE\n",
    "        recon_data, mu, logvar = model(data)\n",
    "        data = data.view(original_shape)\n",
    "        recon_data = recon_data.view(original_shape)\n",
    "        \n",
    "        # Compute loss\n",
    "        rec, kl = model.loss_function(recon_data, data, mu, logvar)\n",
    "        \n",
    "        total_loss = rec + kl * 0.1\n",
    "        # total_loss = rec + kl\n",
    "        total_loss.backward()\n",
    "        train_loss += total_loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tMSE: {:.6f}\\tKL: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                rec.item() / len(data),\n",
    "                kl.item() / len(data)))\n",
    "        # Print image reconstruction examples every few batches\n",
    "        if batch_idx % (args.log_interval * 5) == 0:\n",
    "            example_images(data, recon_data, epoch, batch_idx, args.log_interval)\n",
    "\n",
    "                \n",
    "    train_loss /=  len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss))\n",
    "   \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "    \n",
    "    \n",
    "    os.makedirs(f'/mayo_atlas/home/m296984/MAIN_CHAIN_LIVER_RESULTS/vae_logs_masked_DELETE/{args.log_dir}', exist_ok=True)\n",
    "    torch.save(model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),\n",
    "            '/mayo_atlas/home/m296984/MAIN_CHAIN_LIVER_RESULTS/vae_logs_masked_DELETE/{}/checkpoint_{}.pt'.format(args.log_dir, str(epoch)))\n",
    "    \n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Training model time ({args.epochs}): {execution_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_encoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
