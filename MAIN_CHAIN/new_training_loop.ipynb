{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 09:41:35.976975: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 09:41:37.069158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from custom_dataset import CustomDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "import time \n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from model_8_hdim_edited import Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'patch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda:3\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mcuda \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[39m# ## Data Parallelism\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# if args.cuda and torch.cuda.device_count() > 1:\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m#     print(\"Using\", torch.cuda.device_count(), \"GPUs for Data Parallelism!\")\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#     model = nn.DataParallel(Autoencoder(args.input_size)).to(device)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m# # else:\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m model \u001b[39m=\u001b[39m Autoencoder(args\u001b[39m.\u001b[39;49minput_size)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mnum_workers\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m48\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpin_memory\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m} \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mcuda \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m     29\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     30\u001b[0m     transforms\u001b[39m.\u001b[39mResize((args\u001b[39m.\u001b[39minput_size, args\u001b[39m.\u001b[39minput_size)),\n\u001b[1;32m     31\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m     32\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize((\u001b[39m0.5\u001b[39m,), (\u001b[39m0.5\u001b[39m,))\n\u001b[1;32m     33\u001b[0m ])\n",
      "File \u001b[0;32m/mayo_atlas/home/m296984/MAIN_CHAIN/model_8_hdim_edited.py:27\u001b[0m, in \u001b[0;36mAutoencoder.__init__\u001b[0;34m(self, device, input_channels)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz_dim \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_channels \u001b[39m=\u001b[39m input_channels\n\u001b[0;32m---> 27\u001b[0m image_size \u001b[39m=\u001b[39m patch_size\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_encoder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_channels, image_size, hidden_sizes)\n\u001b[1;32m     31\u001b[0m \u001b[39m# output size depends on input image size, compute the output size\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'patch_size' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" This script is an example of VAE training in PyTorch. The code was adapted from:\n",
    "https://github.com/pytorch/examples/blob/master/vae/main.py \"\"\"\n",
    "\n",
    "## Arguments\n",
    "args = argparse.Namespace(\n",
    "    batch_size=32768,\n",
    "    input_size=16,\n",
    "    epochs=50,\n",
    "    no_cuda=False,\n",
    "    log_interval=500,\n",
    "    model='mse_vae',\n",
    "    log_dir='mse_vae'\n",
    ")\n",
    "\n",
    "\n",
    "## Cuda\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:3\" if args.cuda else \"cpu\")\n",
    "\n",
    "# ## Data Parallelism\n",
    "# if args.cuda and torch.cuda.device_count() > 1:\n",
    "#     print(\"Using\", torch.cuda.device_count(), \"GPUs for Data Parallelism!\")\n",
    "#     model = nn.DataParallel(Autoencoder(args.input_size)).to(device)\n",
    "# # else:\n",
    "model = Autoencoder(args.input_size).to(device)\n",
    "\n",
    "kwargs = {'num_workers': 48, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((args.input_size, args.input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "def random_mask(img_data, device):\n",
    "    # Generate a binary mask with a random percentage of values set to 1\n",
    "    mask_percentage = np.random.uniform(0.05, 0.2)\n",
    "    mask = torch.rand(img_data.shape, device=device) < mask_percentage\n",
    "    # Apply the mask to the random_image to replace values with -1000\n",
    "    dummy_value = -1\n",
    "    masked_image = torch.where(mask, dummy_value, img_data)\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ---------------------------------------------------\n",
    "train_dataset = CustomDataset(data_path='/mayo_atlas/home/m296984/MAIN_CHAIN_LIVER_RESULTS/train_16x16_patches', transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_dataset = CustomDataset(data_path='/mayo_atlas/home/m296984/MAIN_CHAIN_LIVER_RESULTS/test_16x16_patches', transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run VAE\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        # Compute loss\n",
    "        rec, kl = model.module.loss_function(recon_batch, data, mu, logvar)\n",
    "        \n",
    "        total_loss = rec + kl * 0.1\n",
    "        # total_loss = rec + kl \n",
    "        total_loss.backward()\n",
    "        train_loss += total_loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tMSE: {:.6f}\\tKL: {:.6f}\\tlog_sigma: {:.6f}\\tLR: {:.13f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                rec.item() / len(data),\n",
    "                kl.item() / len(data),\n",
    "                model.module.log_sigma if isinstance(model, nn.DataParallel) else model.log_sigma,\n",
    "                scheduler.get_last_lr()[0]))\n",
    "                \n",
    "    train_loss /=  len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(test_loader)):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            rec, kl = model.module.loss_function(recon_batch, data, mu, logvar)\n",
    "            test_loss += rec + kl * 0.1\n",
    "            # test_loss += rec + kl\n",
    "            if i == 2:\n",
    "                n = min(data.size(0), 20)\n",
    "                comparison = torch.cat([data[:n], recon_batch.view(args.batch_size_test, -1, args.input_size, args.input_size)[:n]])\n",
    "                save_image(comparison.cpu(), '/mayo_atlas/home/m296984/MAIN_CHAIN_LIVER_RESULTS/SAVED TEST_IMAGES/{}/reconstruction_{}.png'.format(args.log_dir, str(epoch)), nrow=n)\n",
    "                \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_encoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
