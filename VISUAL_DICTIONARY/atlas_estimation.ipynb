{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Settting the GPU ID to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "\n",
    "from openslide_handler import OpenSlideHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wsi_dimensions(wsi_path):\n",
    "    '''\n",
    "    Function to get the dimensions of the WSI\n",
    "    Args:\n",
    "    wsi_path: Path of the WSI\n",
    "    Returns:\n",
    "    openslide_obj.wsi_dimensions: Dimensions of the WSI\n",
    "    '''\n",
    "    openslide_obj = OpenSlideHandler(wsi_path)\n",
    "    return openslide_obj.wsi_dimensions\n",
    "\n",
    "def load_atlas_data(pkl_files_path):\n",
    "    '''\n",
    "    Loads the atlas data from the pkl files\n",
    "    Args:\n",
    "    pkl_files_path: Folder path of the pkl files\n",
    "    returns:\n",
    "    atlas_data: Dictionary of atlas data\n",
    "    '''\n",
    "    # Initializing the atlas data dictionary\n",
    "    atlas_data = {\n",
    "        'file_names': [],\n",
    "        'load_configs': [],\n",
    "        'image_regions': [],\n",
    "        'embeddings': [],\n",
    "        'masks': [],\n",
    "        'measurements': [],\n",
    "        'selected' : []\n",
    "    }\n",
    "\n",
    "    # Loading the atlas data from the pkl files\n",
    "    for pickel_file in tqdm(os.listdir(pkl_files_path), total=len(os.listdir(pkl_files_path))):\n",
    "        # Checking if the file is a pkl file\n",
    "        if pickel_file.endswith('.pkl'):\n",
    "            with open(os.path.join(pkl_files_path, pickel_file), 'rb') as f:\n",
    "                patch_data = pickle.load(f)\n",
    "\n",
    "            # Appending the data to the atlas data dictionary\n",
    "            atlas_data['file_names'].append([patch_data['file_name']] * len(patch_data['embeddings']))\n",
    "            atlas_data['load_configs'].append(patch_data['load_configs'])\n",
    "            atlas_data['image_regions'].append(patch_data['image_regions'])\n",
    "            atlas_data['embeddings'].append(patch_data['embeddings'])\n",
    "            atlas_data['masks'].append(patch_data['mask'])\n",
    "            atlas_data['measurements'].append(patch_data['measurements'])\n",
    "            atlas_data['selected'].append(np.full(len(patch_data['embeddings']), False))\n",
    "    \n",
    "    # Concatenating the data    \n",
    "    atlas_data['file_names'] = np.concatenate(atlas_data['file_names'])\n",
    "    atlas_data['load_configs'] = np.concatenate(atlas_data['load_configs'])\n",
    "    atlas_data['image_regions'] = np.concatenate(atlas_data['image_regions'])\n",
    "    atlas_data['embeddings'] = np.vstack(atlas_data['embeddings'])\n",
    "    atlas_data['masks'] = np.concatenate(atlas_data['masks'])\n",
    "    atlas_data['measurements'] = np.concatenate(atlas_data['measurements'])\n",
    "    atlas_data['selected'] = np.concatenate(atlas_data['selected'])\n",
    "\n",
    "    # Reforming the measurements and removing the last column which is the ROI and is not needed\n",
    "    atlas_data['measurements'] = np.array([list(x.values()) for x in atlas_data['measurements']])[:, :-1]\n",
    "            \n",
    "    return atlas_data\n",
    "\n",
    "def predict(new_cases, known_cases, known_measurements, vote_method='mean', simil_method='euclidean', top_n=5):\n",
    "    '''\n",
    "    Function to predict the measurements of the new cases\n",
    "    Args:\n",
    "    new_cases: Embeddings of the new cases\n",
    "    known_cases: Embeddings of the known cases\n",
    "    known_measurements: Measurements of the known cases\n",
    "    vote_method: Method to use for voting the measurements\n",
    "    simil_method: Method to use for calculating the similarity between the new and known cases\n",
    "    top_n: Number of top cases to use for voting\n",
    "    Returns:\n",
    "    vote_results: Predicted measurements of the new cases\n",
    "    top_n_indices: Indices of the top cases used for voting\n",
    "    top_n_similarities: Similarities of the top cases used for voting\n",
    "    '''\n",
    "    # Calculating the distances between the new and known cases\n",
    "    # C is the number of new cases\n",
    "    # M is the number of known cases\n",
    "    # N is the embedding dimension\n",
    "    distances = scipy.spatial.distance.cdist(\n",
    "        new_cases, # C x N\n",
    "        known_cases, # M x N\n",
    "        'euclidean'\n",
    "    ) # C x M\n",
    "\n",
    "    # Asserting the dimensions of the distances matches the known measurements\n",
    "    assert distances.shape[1] == known_measurements.shape[0]\n",
    "    \n",
    "    # Adding a small offset to the distances to avoid division by zero\n",
    "    zero_offset = 1e-6\n",
    "    \n",
    "    # Functiions for voting\n",
    "    vote_fns = {\n",
    "        'mean': lambda x, _: np.mean(x, axis=1), # Voting using the mean\n",
    "        'median': lambda x, _: np.median(x, axis=1), # VOting using the median\n",
    "        'max': lambda x, _: np.max(x, axis=1), # Voting using the max\n",
    "        'weighted_mean_distance': (lambda x, y: \n",
    "            np.average(x, axis=1, weights=1/(np.repeat(y, x.shape[2], axis=1).reshape(y.shape[0], y.shape[1], -1)+zero_offset))\n",
    "        ), # Voting using the weighted mean of the distances\n",
    "        'weighted_mean_cosine': (lambda x, y:\n",
    "            np.average(x, axis=1, weights=np.repeat(y, x.shape[2], axis=1).reshape(y.shape[0], y.shape[1], -1))\n",
    "        ), # Voting using the weighted mean of the cosine similarities\n",
    "        'squread_weighted_mean': (lambda x, y:\n",
    "            np.average(x, axis=1, weights=1/((np.repeat(y, x.shape[2], axis=1).reshape(y.shape[0], y.shape[1], -1)+zero_offset)**2))\n",
    "        ), # Voting using the squared weighted mean of the distances\n",
    "    }\n",
    "    \n",
    "    # Sorting the distances and getting the top_n indices\n",
    "    if simil_method == 'euclidean':       \n",
    "        similar_indices = np.argsort(distances, axis=1)\n",
    "    elif simil_method == 'cosine':\n",
    "        similar_indices = np.argsort(distances, axis=1)[:, ::-1]\n",
    "    \n",
    "    # Getting the top_n measurements, indices and distances\n",
    "    top_n_measurements = np.array(known_measurements)[similar_indices][:, :top_n]\n",
    "    top_n_indices = similar_indices[:, :top_n]\n",
    "    top_n_distances = np.sort(distances, axis=1)[:, :top_n]\n",
    "    \n",
    "    # Returning the vote results, top_n indices and top_n distances\n",
    "    return vote_fns[vote_method](top_n_measurements, top_n_distances), top_n_indices, top_n_distances\n",
    "\n",
    "def cross_validation(known_cases, known_measurements, vote_method, simil_method, top_n, n_splits=5):\n",
    "    '''\n",
    "    Function to perform cross validation testing\n",
    "    Args:\n",
    "    known_cases: Embeddings of the known cases\n",
    "    known_measurements: Measurements of the known cases\n",
    "    vote_method: Method to use for voting the measurements\n",
    "    simil_method: Method to use for calculating the similarity between the new and known cases\n",
    "    top_n: Number of top cases to use for voting\n",
    "    n_splits: Number of splits to use for cross validation\n",
    "    Returns:\n",
    "    y_true: True measurements of the known cases\n",
    "    y_pred: Predicted measurements of the known cases\n",
    "    '''    \n",
    "    # Creating the stratified k-fold\n",
    "    stratified_k_fold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    # Getting the labels of the known cases for stratified k-fold\n",
    "    case_labels = np.argmax(known_measurements, axis=1)\n",
    "\n",
    "    # Creating the lists to store the true and predicted measurements\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # Performing the cross validation\n",
    "    for fold_index, (train_index, test_index) in enumerate(stratified_k_fold.split(known_cases, case_labels)):\n",
    "        # Getting the train and test cases and measurements\n",
    "        _train_cases = known_cases[train_index]\n",
    "        _train_measurements = known_measurements[train_index]\n",
    "        \n",
    "        # Getting the test cases and measurements\n",
    "        _test_cases = known_cases[test_index]\n",
    "        _test_measurements = known_measurements[test_index]\n",
    "        \n",
    "        # Asserting the dimensions of the train and test cases and measurements\n",
    "        assert _train_cases.shape[0] == _train_measurements.shape[0]\n",
    "        \n",
    "        # Printing the fold information\n",
    "        print(f'Fold {fold_index+1}/{n_splits} - Train: {_train_cases.shape[0]} - Test: {_test_cases.shape[0]}')\n",
    "        \n",
    "        # Getting the predictions\n",
    "        y_true.append(_test_measurements)\n",
    "        y_pred.append(predict(_test_cases, _train_cases, _train_measurements, vote_method, simil_method, top_n)[0])\n",
    "    \n",
    "    # Concatenating the true and predicted measurements                       \n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "def leave_one_out(known_cases, known_measurements, vote_method, simil_method, top_n):\n",
    "    '''\n",
    "    Function to perform leave one out testing\n",
    "    Args:\n",
    "    known_cases: Embeddings of the known cases\n",
    "    known_measurements: Measurements of the known cases\n",
    "    vote_method: Method to use for voting the measurements\n",
    "    simil_method: Method to use for calculating the similarity between the new and known cases\n",
    "    top_n: Number of top cases to use for voting\n",
    "    Returns:\n",
    "    y_true: True measurements of the known cases\n",
    "    y_pred: Predicted measurements of the known cases\n",
    "    '''\n",
    "    # Getting the number of cases\n",
    "    number_of_cases = len(known_cases)\n",
    "    \n",
    "    # Copying the known cases and measurements to preserve the original\n",
    "    original_known_cases = known_cases.copy()\n",
    "    original_known_measurements = known_measurements.copy()\n",
    "    \n",
    "    # Defining the local job function for multiprocessing\n",
    "    def _local_job(out_index):\n",
    "        # Getting the test case and removing it from the known cases\n",
    "        _test_case = np.expand_dims(original_known_cases[out_index, :], axis=0)\n",
    "        # Removing the test case from the known cases and measurements\n",
    "        _known_cases = np.vstack([original_known_cases[:out_index], original_known_cases[out_index + 1:]])\n",
    "        _known_measurements = np.vstack([original_known_measurements[:out_index], original_known_measurements[out_index + 1:]])\n",
    "        \n",
    "        # Asserting the dimensions of the known cases and measurements\n",
    "        assert len(_known_cases) == len(_known_measurements)\n",
    "        \n",
    "        # Getting the true and predicted measurements\n",
    "        _true = original_known_measurements[out_index, :]\n",
    "        _pred, _, _ = predict(_test_case, _known_cases, _known_measurements, vote_method, simil_method, top_n)\n",
    "        \n",
    "        # Returning the true and predicted measurements          \n",
    "        return _true, _pred\n",
    "    \n",
    "    # Performing the leave one out testing using multiprocessing\n",
    "    with Pool(64) as pool:\n",
    "        tested = list(tqdm(\n",
    "            pool.imap(_local_job, range(number_of_cases)),\n",
    "            total=number_of_cases,\n",
    "            desc='Testing'\n",
    "            ))\n",
    "    \n",
    "    # Getting the true and predicted measurements\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for _true, _pred in tested:\n",
    "        y_true.append(_true)\n",
    "        y_pred.append(_pred)\n",
    "    \n",
    "    return np.array(y_true), np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the network, patch size and magnification\n",
    "network = 'kimianet'\n",
    "patch_size = 128\n",
    "magnification = 40\n",
    "\n",
    "# Setting the labels\n",
    "annot_labels = ['steatosis', 'ballooning',\n",
    "                'mallory', 'inflammation', 'fibrosis', 'roi']\n",
    "\n",
    "# Setting the path to the atlas\n",
    "atlas_path = f'/mayo_atlas/home/m276983/atlases/liver_estimate_atlases/{network}_{patch_size}_{magnification}X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the atlas data\n",
    "atlas_data = load_atlas_data(atlas_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of each array in atlas_data\n",
    "for key, value in atlas_data.items():\n",
    "    print(f'{key}: {value.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the cases with measurements above a threshold\n",
    "selection_threshold = 0.0\n",
    "sample_number = 10000\n",
    "\n",
    "# resetting the selected array\n",
    "atlas_data['selected'] = np.full(atlas_data['measurements'].shape[0], False)\n",
    "\n",
    "# Selecting the cases with measurements above the threshold in each label\n",
    "for l_index, label in enumerate(annot_labels[:-1]):\n",
    "    # Finding the indices of the cases with measurements above the threshold\n",
    "    case_indices = np.where(atlas_data['measurements'][:, l_index] > selection_threshold)[0]\n",
    "    label_case_number = len(case_indices)\n",
    "    # Checking if the number of cases is greater than the sample number\n",
    "    if label_case_number > sample_number:\n",
    "        selected_indices = np.random.choice(case_indices, sample_number, replace=False)\n",
    "    else:\n",
    "        selected_indices = case_indices\n",
    "    # Setting the selected indices to True\n",
    "    atlas_data['selected'][selected_indices] = True\n",
    "    # Printing the number of cases with measurements above the threshold   \n",
    "    print(f'selecting {len(selected_indices)} of {label_case_number} cases for {label}')\n",
    "    \n",
    "# adding a number of cases with no finding\n",
    "normal_indices = np.where(~atlas_data['measurements'].any(axis=1))[0]\n",
    "normal_case_number = len(normal_indices)\n",
    "selected_normal_indices = np.random.choice(normal_indices, sample_number, replace=False)\n",
    "print(f'selecting {len(selected_normal_indices)} of {normal_case_number} cases for no finding')\n",
    "atlas_data['selected'][selected_normal_indices] = True\n",
    "\n",
    "print(f'Number of selected cases in total: {atlas_data[\"selected\"].sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of each array in atlas_data\n",
    "for key, value in atlas_data.items():\n",
    "    print(f'{key}: {value[atlas_data[\"selected\"]].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the similarity and voting methods\n",
    "vote_method = 'weighted_mean_distance'\n",
    "similarity_method = 'euclidean'\n",
    "top_n = 100\n",
    "n_splits = 5 \n",
    "\n",
    "# Testing using leave one out\n",
    "# y_true, y_pred = leave_one_out(\n",
    "#     atlas_data['embeddings'][atlas_data['selected']],\n",
    "#     atlas_data['measurements'][atlas_data['selected']],\n",
    "#     vote_method=vote_method,\n",
    "#     simil_method=similarity_method,\n",
    "#     top_n=top_n\n",
    "#     )\n",
    "\n",
    "# Testing using cross validation\n",
    "y_true, y_pred = cross_validation(\n",
    "    atlas_data['embeddings'][atlas_data['selected']],\n",
    "    atlas_data['measurements'][atlas_data['selected']],\n",
    "    vote_method=vote_method,\n",
    "    simil_method=similarity_method,\n",
    "    top_n=top_n,\n",
    "    n_splits=n_splits\n",
    ")\n",
    "\n",
    "# Checking the shape of y_true and y_pred\n",
    "print(y_true.shape, y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the mean absolute error\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred), axis=0)\n",
    "\n",
    "# function to calculate the mean squared error\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean(np.square(y_true - y_pred), axis=0)\n",
    "\n",
    "# function to calculate the standard deviation of the absolute error\n",
    "def std_absolute_error(y_true, y_pred):\n",
    "    return np.std(np.abs(y_true - y_pred), axis=0)\n",
    "\n",
    "# Printing the mean absolute error, standard deviation of absolute error and r2 score for each label\n",
    "for l_index, label in enumerate(annot_labels[:-1]):\n",
    "    mse = mean_squared_error(y_true[:, l_index], y_pred[:, l_index])\n",
    "    mae = mean_absolute_error(y_true[:, l_index], y_pred[:, l_index])\n",
    "    stdae = std_absolute_error(y_true[:, l_index], y_pred[:, l_index])\n",
    "    r_2_score = metrics.r2_score(y_true[:, l_index], y_pred[:, l_index])\n",
    "    print(\n",
    "        f'{label:<15}: Mean Absolute Error: {mae:.3f}, Standard Deviation of Absolute Error: {stdae:.5f}, r2 score: {r_2_score:.3f}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some example cases\n",
    "n_samples = 5\n",
    "\n",
    "# Selecting the indices of the example cases\n",
    "sample_indices = np.random.choice(range(sum(atlas_data['selected'])), n_samples, replace=False)\n",
    "\n",
    "# Predicting the measurements of the example cases\n",
    "sample_y_pred, top_indices, top_distances = predict(\n",
    "    atlas_data['embeddings'][atlas_data['selected']][sample_indices], \n",
    "    np.delete(atlas_data['embeddings'][atlas_data['selected']], sample_indices, axis=0),\n",
    "    np.delete(atlas_data['measurements'][atlas_data['selected']], sample_indices, axis=0),\n",
    "    vote_method,\n",
    "    similarity_method,\n",
    "    top_n\n",
    "    )\n",
    "\n",
    "# Selecting the true measurements of the example cases\n",
    "sample_y_true = atlas_data['measurements'][atlas_data['selected']][sample_indices]\n",
    "\n",
    "print(sample_y_true.shape, sample_y_pred.shape)\n",
    "\n",
    "# Plotting the example cases\n",
    "for i_index, sample_index in enumerate(sample_indices):\n",
    "    _file_name = atlas_data['file_names'][atlas_data['selected']][sample_index]\n",
    "    _load_config = atlas_data['load_configs'][atlas_data['selected']][sample_index]\n",
    "    _image_region = atlas_data['image_regions'][atlas_data['selected']][sample_index]\n",
    "    _masks = atlas_data['masks'][atlas_data['selected']][sample_index]\n",
    "    _measurement = atlas_data['measurements'][atlas_data['selected']][sample_index]\n",
    "    _y_true = sample_y_true[i_index]\n",
    "    _y_pred = sample_y_pred[i_index]\n",
    "    _top_5_indices = top_indices[i_index][:5]\n",
    "    _top_5_distances = top_distances[i_index][:5]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "\n",
    "    # Remove grids from subplots\n",
    "    for ax in axs.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "    axs[0, 0].imshow(_image_region)\n",
    "    axs[0, 0].set_title('image_region')\n",
    "\n",
    "    for l_index, (label, measure) in enumerate(zip(annot_labels[:-1], _measurement[:-1])):\n",
    "        axs[0, l_index + 1].imshow(_masks[label], cmap='gray', vmin=0, vmax=1)\n",
    "        axs[0, l_index + 1].set_title(\n",
    "            f'{label}: {measure:.2f}\\n'\n",
    "            f'true: {_y_true[l_index]:.2f}\\n'\n",
    "            f'pred: {_y_pred[l_index]:.2f}'\n",
    "\n",
    "            )\n",
    "        \n",
    "    # Selecting the similar patches by removing the sample indices from the selected indices\n",
    "    simil_patches = np.delete(atlas_data['image_regions'][atlas_data['selected']], sample_indices, axis=0)[_top_5_indices]\n",
    "    simil_file_names = np.delete(atlas_data['file_names'][atlas_data['selected']], sample_indices, axis=0)[_top_5_indices]\n",
    "    simil_load_configs = np.delete(atlas_data['load_configs'][atlas_data['selected']], sample_indices, axis=0)[_top_5_indices]\n",
    "    simil_measurements = np.delete(atlas_data['measurements'][atlas_data['selected']], sample_indices, axis=0)[_top_5_indices]\n",
    "    \n",
    "    for l_index, (patch, measurements, distance, file_name, load_config) in enumerate(zip(\n",
    "        simil_patches,\n",
    "        simil_measurements,\n",
    "        _top_5_distances,\n",
    "        simil_file_names,\n",
    "        simil_load_configs)):\n",
    "        axs[1, l_index].imshow(patch)\n",
    "        axs[1, l_index].set_title(\n",
    "            f'file: {file_name}\\n'\n",
    "            + f'location: {load_config[\"location\"]}\\n'\n",
    "            + '\\n'.join([f\"{label}: {measure:.2f}\" for label, measure in zip(annot_labels[:-1], measurements[:-1])])\n",
    "            + f'\\ndistance: {distance:.2f}'\n",
    "            )\n",
    "\n",
    "    # Adjust subplot spacing\n",
    "    plt.subplots_adjust(wspace=0.1)\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    fig.suptitle(\n",
    "        f'Image region from {_file_name}.svs with load config {_load_config}')\n",
    "    \n",
    "    # saving the figure with the name reflecting patch size, network and magnification\n",
    "    plt.savefig(f'sample_patch_{patch_size}_{network}_{magnification}_{i_index}.png', bbox_inches = 'tight')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
